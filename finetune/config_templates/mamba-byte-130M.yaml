model_name: mamba-byte-base-130M-53k
n_layer: 24
d_model: 768
chunk_size: 53248
dataset: /some/dataset/dir
batch_size: 1
gradient_accumulation_steps: 1
max_steps: -1
num_train_epochs: 15
logging_steps: 25
warmup_steps: 20
save_steps: 100
save_total_limit: 3
learning_rate: 0.0005
optimizer: paged_adamw_8bit
scheduler: linear
