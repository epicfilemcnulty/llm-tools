model_name: phind-finetune
base_model: /storage/models/FP16/Phind-CodeLlama-34B-v2
max_len: 2048
group_by_length: False
lora_rank: 64
lora_alpha: 128
lora_dropout: 0.1
lora_bias: none
target_modules:
- q_proj
- k_proj
- v_proj
- o_proj
- up_proj
- down_proj
- gate_proj
modules_to_save: []
dataset: /path/to/dataset
dataset_test_split: 0.0001
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
max_steps: 1000
warmup_steps: 20
save_steps: 50
eval_steps: 25
logging_steps: 5
save_total_limit: 10
num_train_epochs: 1
learning_rate: 0.00001
weight_decay: 0.001
optimizer: paged_adamw_8bit
